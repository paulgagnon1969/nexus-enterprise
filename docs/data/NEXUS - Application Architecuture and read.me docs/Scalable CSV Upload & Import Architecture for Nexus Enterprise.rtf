{\rtf1\ansi\ansicpg1252\cocoartf2867
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\froman\fcharset0 Times-Bold;\f1\froman\fcharset0 Times-Roman;\f2\fmodern\fcharset0 Courier;
}
{\colortbl;\red255\green255\blue255;\red0\green0\blue0;}
{\*\expandedcolortbl;;\cssrgb\c0\c0\c0;}
{\*\listtable{\list\listtemplateid1\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat0\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid1\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid1}
{\list\listtemplateid2\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat0\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid101\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid2}
{\list\listtemplateid3\listhybrid{\listlevel\levelnfc0\levelnfcn0\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{decimal\}}{\leveltext\leveltemplateid201\'01\'00;}{\levelnumbers\'01;}\fi-360\li720\lin720 }{\listname ;}\listid3}
{\list\listtemplateid4\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat0\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid301\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid4}
{\list\listtemplateid5\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat0\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid401\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid5}
{\list\listtemplateid6\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat0\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid501\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid6}
{\list\listtemplateid7\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat0\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid601\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid7}
{\list\listtemplateid8\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat0\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid701\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid8}
{\list\listtemplateid9\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat0\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid801\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid9}
{\list\listtemplateid10\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat0\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid901\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid10}
{\list\listtemplateid11\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat0\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid1001\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid11}
{\list\listtemplateid12\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat0\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid1101\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid12}
{\list\listtemplateid13\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat0\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid1201\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid13}
{\list\listtemplateid14\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat0\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid1301\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid14}
{\list\listtemplateid15\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat0\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid1401\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid15}
{\list\listtemplateid16\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat0\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid1501\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid16}
{\list\listtemplateid17\listhybrid{\listlevel\levelnfc0\levelnfcn0\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{decimal\}}{\leveltext\leveltemplateid1601\'01\'00;}{\levelnumbers\'01;}\fi-360\li720\lin720 }{\listname ;}\listid17}}
{\*\listoverridetable{\listoverride\listid1\listoverridecount0\ls1}{\listoverride\listid2\listoverridecount0\ls2}{\listoverride\listid3\listoverridecount0\ls3}{\listoverride\listid4\listoverridecount0\ls4}{\listoverride\listid5\listoverridecount0\ls5}{\listoverride\listid6\listoverridecount0\ls6}{\listoverride\listid7\listoverridecount0\ls7}{\listoverride\listid8\listoverridecount0\ls8}{\listoverride\listid9\listoverridecount0\ls9}{\listoverride\listid10\listoverridecount0\ls10}{\listoverride\listid11\listoverridecount0\ls11}{\listoverride\listid12\listoverridecount0\ls12}{\listoverride\listid13\listoverridecount0\ls13}{\listoverride\listid14\listoverridecount0\ls14}{\listoverride\listid15\listoverridecount0\ls15}{\listoverride\listid16\listoverridecount0\ls16}{\listoverride\listid17\listoverridecount0\ls17}}
\margl1440\margr1440\vieww11520\viewh8400\viewkind0
\deftab720
\pard\pardeftab720\sa321\partightenfactor0

\f0\b\fs48 \cf0 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Scalable CSV Upload & Import Architecture for Nexus Enterprise\
\pard\pardeftab720\sa298\partightenfactor0

\fs36 \cf0 Problem Statement\
\pard\pardeftab720\sa240\partightenfactor0

\f1\b0\fs24 \cf0 Nexus currently supports several CSV-based imports (Xact RAW, Xact Components, Golden PETL, Golden Components). In deployed environments (Vercel web \'e2\'86\'92 Cloud Run API/worker), the existing Xact import flow relies on file paths local to the web server, which do not exist on the API container. Import jobs are processed as a single worker task per file, reading the entire CSV into memory and performing all DB work in one pass.\
We need a robust, scalable architecture that can handle thousands of imports per day, with support for true intra-file parallelism (e.g., 10+ workers per file), while remaining correct, observable, and cost-efficient.\
\pard\pardeftab720\sa298\partightenfactor0

\f0\b\fs36 \cf0 Current State (Relevant Pieces)\
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls1\ilvl0
\f1\b0\fs24 \cf0 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Web (Next.js on Vercel): 
\f2\fs26 /projects/import
\f1\fs24  page uploads CSVs to a Next API route under 
\f2\fs26 /api/projects/[projectId]/import-xact
\f1\fs24 . The route writes the file to a local temp path and, in prod, calls 
\f2\fs26 POST /projects/:id/import-xact
\f1\fs24  on the API with 
\f2\fs26 \{ csvPath \}
\f1\fs24  in JSON.\
\ls1\ilvl0\kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 API (NestJS on Cloud Run): For local dev: 
\f2\fs26 /projects/:projectId/import-jobs/xact-raw
\f1\fs24  enqueues an 
\f2\fs26 ImportJob
\f1\fs24  and the worker processes it asynchronously. For prod: 
\f2\fs26 /projects/:id/import-xact
\f1\fs24  directly calls 
\f2\fs26 importXactCsvForProject(\{ projectId, csvPath, importedByUserId \})
\f1\fs24 . 
\f2\fs26 importXactCsvForProject
\f1\fs24  reads the full CSV from disk, parses it synchronously, and performs all DB work (RawXactRow, Sow, SowItem, particles, Golden sync) in a single function invocation.\
\ls1\ilvl0\kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Worker (BullMQ + Redis): Single queue 
\f2\fs26 IMPORT_QUEUE_NAME
\f1\fs24  with 
\f2\fs26 IMPORT_WORKER_CONCURRENCY
\f1\fs24  controlling concurrency at the job (file) level. 
\f2\fs26 ImportJob
\f1\fs24  model tracks status/progress for uploads such as Golden PETL, Golden Components, and Xact.\
\ls1\ilvl0\kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Existing design doc "Chunked, Parallel CSV Import Architecture for Nexus Enterprise" already defines a generalized pattern for parent/child (chunk) jobs and proposes applying it first to Golden Components, then Golden PETL, then Xact RAW/Components.\
\pard\pardeftab720\sa240\partightenfactor0
\cf0 Pain points:\
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls2\ilvl0\cf0 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 In prod, 
\f2\fs26 csvPath
\f1\fs24  from Vercel is not visible to Cloud Run containers.\
\ls2\ilvl0\kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Whole-file, single-job imports do not scale well for very large files or thousands of daily imports.\
\ls2\ilvl0\kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 We are not yet standardizing on an object-storage-first upload path.\
\pard\pardeftab720\sa298\partightenfactor0

\f0\b\fs36 \cf0 Target Architecture (High-Level)\
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls3\ilvl0
\fs24 \cf0 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	1	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Object storage as the source of truth for uploads
\f1\b0 : All web-initiated CSV uploads are stored in durable object storage (e.g., GCS bucket) instead of ephemeral local disk inside Vercel/Cloud Run. The browser uploads directly to storage via a signed URL (no large file bodies through Vercel \'e2\'86\'92 API \'e2\'86\'92 worker). The API (and worker) only ever see URIs like 
\f2\fs26 gs://nexus-uploads/xact-raw/<company>/<project>/<timestamp>.csv
\f1\fs24 .\
\ls3\ilvl0
\f0\b \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	2	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Unified ImportJob model with parent + chunk jobs
\f1\b0 : Keep a single 
\f2\fs26 ImportJob
\f1\fs24  row per uploaded file (existing model). Extend it with chunk metadata and optional 
\f2\fs26 metaJson
\f1\fs24  describing planning/strategy. Use a single BullMQ queue with a discriminated payload (
\f2\fs26 parent
\f1\fs24  vs 
\f2\fs26 chunk
\f1\fs24 ) as in the existing Chunked Import doc.\
\ls3\ilvl0
\f0\b \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	3	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Tiered import pipeline per file
\f1\b0 : Phase 0 (upload): browser uploads CSV to object storage and notifies API, which creates the 
\f2\fs26 ImportJob
\f1\fs24  with 
\f2\fs26 fileUri
\f1\fs24  and enqueues a parent job. Phase 1 (planning): parent job validates the file, determines parallelization strategy and 
\f2\fs26 chunkCount
\f1\fs24 , and enqueues N chunk jobs. Phase 2 (chunk processing): chunk jobs run in parallel (10+ per file if desired), each reading only its partition from storage and performing conflict-free writes. Phase 3 (finalization): once all chunks have succeeded, a finalization step updates summary data and marks the parent 
\f2\fs26 ImportJob
\f1\fs24  as 
\f2\fs26 SUCCEEDED
\f1\fs24 .\
\ls3\ilvl0
\f0\b \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	4	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Specialization by import type
\f1\b0 : Golden Components and Golden PETL follow the hash-partition chunking and wipe/rebuild semantics defined in the existing doc. Xact RAW and Xact Components use a slightly different split: chunked ingestion of raw rows and components, followed by a single modeling/finalization pass (units, particles, SOW, Golden sync).\
\ls3\ilvl0
\f0\b \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	5	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Horizontal scalability and observability
\f1\b0 : Multiple worker instances (Cloud Run services) subscribed to the same BullMQ queue, each with configurable concurrency. Import-level metrics: job counts, durations, per-import chunk counts and progress, per-company rate limiting. UI surfaces 
\f2\fs26 ImportJob.status
\f1\fs24 , 
\f2\fs26 progress
\f1\fs24 , 
\f2\fs26 totalChunks
\f1\fs24 , 
\f2\fs26 completedChunks
\f1\fs24 , and high-level messages.\
\pard\pardeftab720\sa298\partightenfactor0

\f0\b\fs36 \cf0 Proposed Changes by Layer\
\pard\pardeftab720\sa280\partightenfactor0

\fs28 \cf0 1. Upload & Storage Layer (Web + API)\
\pard\pardeftab720\sa319\partightenfactor0

\fs24 \cf0 1.1 Move to signed URL uploads\
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls4\ilvl0
\f1\b0 \cf0 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Add an authenticated API endpoint (Nest): 
\f2\fs26 POST /projects/:id/uploads/xact-raw-url
\f1\fs24  (and similar for other import types): Validates the caller and project. Generates a short-lived signed upload URL for object storage, scoped to a specific key prefix (company/project/import type). Returns 
\f2\fs26 \{ uploadUrl, fileUri \}
\f1\fs24 , where 
\f2\fs26 fileUri
\f1\fs24  is the canonical storage URI that will be stored on 
\f2\fs26 ImportJob
\f1\fs24 .\
\ls4\ilvl0\kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 In the Next.js web app: Replace the current 
\f2\fs26 /api/projects/[projectId]/import-xact
\f1\fs24  route for prod with a 2-step flow: Call the signed-URL endpoint to obtain 
\f2\fs26 \{ uploadUrl, fileUri \}
\f1\fs24 . 
\f2\fs26 PUT
\f1\fs24  the CSV directly to 
\f2\fs26 uploadUrl
\f1\fs24  from the browser (with progress UI). After successful upload, call a lightweight API endpoint like 
\f2\fs26 POST /projects/:id/import-jobs/xact-raw-from-uri
\f1\fs24  with 
\f2\fs26 \{ fileUri \}
\f1\fs24  to create the 
\f2\fs26 ImportJob
\f1\fs24  and enqueue the parent job.\
\pard\pardeftab720\sa240\partightenfactor0
\cf0 Benefits: No large request bodies traversing Vercel \'e2\'86\'92 API \'e2\'86\'92 worker. File is accessible from any Cloud Run instance or local dev via the same URI. Upload reliability improves (resumable uploads, retries handled at storage layer).\
\pard\pardeftab720\sa319\partightenfactor0

\f0\b \cf0 1.2 Local dev compatibility\
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls5\ilvl0
\f1\b0 \cf0 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 For local dev, we can support two modes: Direct local disk (simpler): keep the existing 
\f2\fs26 csvPath
\f1\fs24  flow for localhost-only scenarios where API and web share a filesystem. Local object storage-like behavior: use a dev GCS bucket or MinIO, but still go through the same signed-URL pipeline, ensuring parity with prod.\
\ls5\ilvl0\kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Implementation detail: the web route can detect 
\f2\fs26 isLocalApi
\f1\fs24  and either: Use the existing 
\f2\fs26 /api/projects/[projectId]/import-xact
\f1\fs24  path with 
\f2\fs26 csvPath
\f1\fs24  (no change to local dev UX), or Use the new signed-URL flow even in dev, but with a 
\f2\fs26 fileUri
\f1\fs24  that points to a dev bucket.\
\pard\pardeftab720\sa280\partightenfactor0

\f0\b\fs28 \cf0 2. ImportJob & Queue Protocol\
\pard\pardeftab720\sa319\partightenfactor0

\fs24 \cf0 2.1 ImportJob schema extension\
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls6\ilvl0
\f1\b0 \cf0 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Extend the 
\f2\fs26 ImportJob
\f1\fs24  model: 
\f2\fs26 fileUri String?
\f1\fs24  (canonical storage URI for the uploaded file), 
\f2\fs26 totalChunks Int?
\f1\fs24  (total number of chunk jobs planned for this import), 
\f2\fs26 completedChunks Int?
\f1\fs24  (number of chunk jobs that have completed successfully), 
\f2\fs26 metaJson Json?
\f1\fs24  (planner/strategy metadata).\
\ls6\ilvl0\kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Maintain existing fields: 
\f2\fs26 status
\f1\fs24 , 
\f2\fs26 progress
\f1\fs24 , 
\f2\fs26 resultJson
\f1\fs24 , 
\f2\fs26 errorJson
\f1\fs24 , etc.\
\pard\pardeftab720\sa319\partightenfactor0

\f0\b \cf0 2.2 BullMQ job payload\
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls7\ilvl0
\f1\b0 \cf0 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Use the discriminated union from the existing Chunked Import doc: Parent job payload: 
\f2\fs26 \{ kind: "parent"; importJobId: string \}
\f1\fs24 . Chunk job payload: 
\f2\fs26 \{ kind: "chunk"; importJobId: string; chunkIndex: number; chunkCount: number; strategy: string; payload: any \}
\f1\fs24 .\
\ls7\ilvl0\kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Worker routing: If 
\f2\fs26 kind === "chunk"
\f1\fs24  \'e2\'86\'92 
\f2\fs26 processImportChunk
\f1\fs24 . Else \'e2\'86\'92 
\f2\fs26 processImportJob
\f1\fs24  (parent).\
\pard\pardeftab720\sa280\partightenfactor0

\f0\b\fs28 \cf0 3. Import Type Pipelines\
\pard\pardeftab720\sa319\partightenfactor0

\fs24 \cf0 3.1 Golden Components & PETL (reuse existing design)\
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls8\ilvl0
\f1\b0 \cf0 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 For Golden components and PETL, follow the already-specified plan: Parent job reads the full CSV once from storage. Partitions into N bucket CSVs based on a stable hash over business keys (e.g., (Cat, Sel, ComponentCode)). Writes these chunk CSVs back to storage under predictable names. Enqueues N chunk jobs for each bucket. Each chunk job performs local writes (delete-many already done by parent) with 
\f2\fs26 createMany
\f1\fs24  and 
\f2\fs26 skipDuplicates
\f1\fs24 .\
\ls8\ilvl0\kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 This establishes and validates the infrastructure for chunk jobs, parent metadata, and progress updates.\
\pard\pardeftab720\sa319\partightenfactor0

\f0\b \cf0 3.2 Xact RAW (XACT_RAW) \'e2\'80\'93 scalable, parallel design\
\pard\pardeftab720\sa240\partightenfactor0

\f1\b0 \cf0 Xact RAW imports are more complex because they involve both ingestion (RawXactRow) and modeling (units, particles, Sow, SowItems, Golden sync). To scale, we separate these concerns.\
\pard\pardeftab720\sa332\partightenfactor0

\f0\b\fs20 \cf0 3.2.1 Phase 1 \'e2\'80\'93 Ingestion planning (parent)\
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls9\ilvl0
\f1\b0\fs24 \cf0 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 For 
\f2\fs26 ImportJobType.XACT_RAW
\f1\fs24 , the parent job will: Read a small prefix of the file from storage to validate headers and basic format. Determine an approximate row count and file size (either via storage metadata or a single streaming pass that counts lines). Choose a 
\f2\fs26 chunkCount
\f1\fs24  based on row count and environment. Decide on partitioning strategy (simplest: contiguous line ranges; advanced: hash-based on logical item keys). Write per-chunk descriptors to storage, e.g. 
\f2\fs26 gs://.../xact-raw/<importJobId>/manifest.json
\f1\fs24 . Create or reserve a new 
\f2\fs26 EstimateVersion
\f1\fs24  row, but delay heavy SOW work until after ingestion. Update 
\f2\fs26 ImportJob
\f1\fs24  with 
\f2\fs26 totalChunks
\f1\fs24 , 
\f2\fs26 completedChunks = 0
\f1\fs24 , 
\f2\fs26 metaJson
\f1\fs24 , and set initial 
\f2\fs26 progress
\f1\fs24  (~10). Enqueue N chunk jobs, each with a payload indicating its assigned range or partition key set.\
\pard\pardeftab720\sa332\partightenfactor0

\f0\b\fs20 \cf0 3.2.2 Phase 2 \'e2\'80\'93 Chunked ingestion\
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls10\ilvl0
\f1\b0\fs24 \cf0 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Each chunk job will: Read only its assigned range from the file in storage (using range reads or a pre-split chunk file). Stream-parse CSV rows (avoid 
\f2\fs26 csv-parse/sync
\f1\fs24  on the entire file) and map them into 
\f2\fs26 RawXactRow
\f1\fs24  records. Perform batched 
\f2\fs26 createMany
\f1\fs24  calls with a tuned batch size (e.g., 5,000\'e2\'80\'9310,000 rows per batch), keeping transactions reasonably small. NOT create SOW items, logical items, or units yet\'e2\'80\'94only raw rows and perhaps minimal per-chunk aggregates.\
\ls10\ilvl0\kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Concurrency and safety: Each chunk writes to 
\f2\fs26 RawXactRow
\f1\fs24  with a unique 
\f2\fs26 (estimateVersionId, lineNo)
\f1\fs24  or surrogate 
\f2\fs26 id
\f1\fs24 , so no cross-chunk write conflicts. No chunk modifies shared higher-level structures.\
\pard\pardeftab720\sa332\partightenfactor0

\f0\b\fs20 \cf0 3.2.3 Phase 3 \'e2\'80\'93 Modeling/finalization (single job)\
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls11\ilvl0
\f1\b0\fs24 \cf0 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 After all ingestion chunks have completed (
\f2\fs26 completedChunks === totalChunks
\f1\fs24 ), the system performs modeling in one or a small number of jobs: Build units and particles from the union of 
\f2\fs26 RawXactRow
\f1\fs24  grouped by 
\f2\fs26 GroupDescription
\f1\fs24  (and GroupCode), using existing logic but now operating purely from DB rows. Construct 
\f2\fs26 Sow
\f1\fs24  and 
\f2\fs26 SowItem
\f1\fs24  entries for all raw rows. Run any Golden price list sync logic (e.g., 
\f2\fs26 updateGoldenFromEstimate
\f1\fs24 ).\
\ls11\ilvl0\kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Implementation options: Trigger finalization directly from the last chunk job in a transaction-safe manner. Or enqueue a separate 
\f2\fs26 kind: "chunk" | strategy: "XACT_RAW:finalize"
\f1\fs24  job to keep responsibilities clear.\
\ls11\ilvl0\kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Progress reporting: Use 0\'e2\'80\'9390% for ingestion (
\f2\fs26 progress = 10 + 70 * completedChunks / totalChunks
\f1\fs24 ). Reserve 90\'e2\'80\'93100% for modeling/finalization, updating 
\f2\fs26 ImportJob
\f1\fs24  as that phase completes.\
\pard\pardeftab720\sa319\partightenfactor0

\f0\b \cf0 3.3 Xact Components (XACT_COMPONENTS) \'e2\'80\'93 parallel design\
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls12\ilvl0
\f1\b0 \cf0 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Once Xact RAW ingestion and modeling are complete for an 
\f2\fs26 estimateVersionId
\f1\fs24 , Xact Components imports can follow a similar chunking approach: Parent job for 
\f2\fs26 XACT_COMPONENTS
\f1\fs24  reads the components CSV from storage, builds a manifest, and partitions into chunk files or logical ranges. Chunk jobs read their assigned portions of the components CSV and create 
\f2\fs26 RawComponentRow
\f1\fs24  and 
\f2\fs26 ComponentSummary
\f1\fs24  entries in batches. A final allocation job (
\f2\fs26 allocateComponentsForEstimate
\f1\fs24 ) can run once all chunk jobs succeed, using existing allocation logic.\
\pard\pardeftab720\sa280\partightenfactor0

\f0\b\fs28 \cf0 4. Worker Scaling & Ops\
\pard\pardeftab720\sa319\partightenfactor0

\fs24 \cf0 4.1 Horizontal workers\
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls13\ilvl0
\f1\b0 \cf0 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Deploy the worker as a separate Cloud Run service (or multiple instances) reading from the same Redis-backed BullMQ queue.\
\ls13\ilvl0\kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Use environment variables for: 
\f2\fs26 IMPORT_WORKER_CONCURRENCY
\f1\fs24  (per-instance parallelism), 
\f2\fs26 MAX_IMPORT_CHUNKS
\f1\fs24  and 
\f2\fs26 TARGET_ROWS_PER_CHUNK
\f1\fs24  (to tune import chunking across environments).\
\ls13\ilvl0\kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 With chunking, scaling to "10+ workers per file" becomes a function of: Choosing 
\f2\fs26 chunkCount >= 10
\f1\fs24  for large files. Having enough worker instances and concurrency to process many chunk jobs simultaneously.\
\pard\pardeftab720\sa319\partightenfactor0

\f0\b \cf0 4.2 Rate limiting and backpressure\
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls14\ilvl0
\f1\b0 \cf0 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Add simple per-company limits in the API when creating new 
\f2\fs26 ImportJob
\f1\fs24 s: E.g., at most N concurrent imports of each type per company. Reject or queue additional requests with a clear error message.\
\ls14\ilvl0\kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Monitor queue depth and worker lag via BullMQ metrics or custom logging.\
\pard\pardeftab720\sa280\partightenfactor0

\f0\b\fs28 \cf0 5. Frontend & UX\
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls15\ilvl0
\f1\b0\fs24 \cf0 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 The web UI already polls 
\f2\fs26 /import-jobs/:id
\f1\fs24  and shows script-window style logs.\
\ls15\ilvl0\kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Extend the DTOs to surface: 
\f2\fs26 fileUri
\f1\fs24  (for debugging/admin views only), 
\f2\fs26 totalChunks
\f1\fs24  and 
\f2\fs26 completedChunks
\f1\fs24 , Distinct messages for planning vs ingestion vs finalization phases.\
\ls15\ilvl0\kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 For large imports, show more informative progress, e.g.: "Planning import (analyzing file)\'e2\'80\'a6", "Importing chunks 3 of 12\'e2\'80\'a6", "Finalizing estimate model\'e2\'80\'a6".\
\pard\pardeftab720\sa280\partightenfactor0

\f0\b\fs28 \cf0 6. Safety, Idempotency, and Failure Modes\
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls16\ilvl0
\f1\b0\fs24 \cf0 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Parent jobs must be 
\f0\b idempotent
\f1\b0 : If a parent job is retried by BullMQ, it should detect an existing 
\f2\fs26 ImportJob
\f1\fs24  in RUNNING/SUCCEEDED state and either short-circuit or re-enqueue missing chunks without duplicating work.\
\ls16\ilvl0\kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Chunk jobs should be 
\f0\b at-least-once safe
\f1\b0 : If a chunk is retried, its writes should either be idempotent (e.g., via 
\f2\fs26 skipDuplicates
\f1\fs24 ) or keyed so that duplicates are impossible.\
\ls16\ilvl0\kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Failure behavior: Any chunk failure marks the parent 
\f2\fs26 ImportJob
\f1\fs24  as 
\f2\fs26 FAILED
\f1\fs24  (existing behavior), with structured 
\f2\fs26 errorJson
\f1\fs24  including 
\f2\fs26 chunkIndex
\f1\fs24 , 
\f2\fs26 strategy
\f1\fs24 , and a summary of the error. Admin tooling or CLI helpers can allow re-running a failed import as a fresh 
\f2\fs26 ImportJob
\f1\fs24  referencing the same 
\f2\fs26 fileUri
\f1\fs24 .\
\pard\pardeftab720\sa298\partightenfactor0

\f0\b\fs36 \cf0 Implementation Phases\
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls17\ilvl0
\fs24 \cf0 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	1	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Storage and upload refactor
\f1\b0 : Implement signed URL endpoints and browser-side upload flow. Add 
\f2\fs26 fileUri
\f1\fs24  to 
\f2\fs26 ImportJob
\f1\fs24  and stop using 
\f2\fs26 csvPath
\f1\fs24  in prod.\
\ls17\ilvl0
\f0\b \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	2	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Golden Components chunking (Phase 1 from existing doc)
\f1\b0 : Implement parent+chunk pattern for 
\f2\fs26 PRICE_LIST_COMPONENTS
\f1\fs24  using object storage, validating the general approach.\
\ls17\ilvl0
\f0\b \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	3	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Golden PETL chunking
\f1\b0 : Apply similar pattern for 
\f2\fs26 PRICE_LIST
\f1\fs24 , ensuring correctness of wipe-and-rebuild semantics.\
\ls17\ilvl0
\f0\b \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	4	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Xact RAW ingestion refactor
\f1\b0 : Separate ingestion of 
\f2\fs26 RawXactRow
\f1\fs24  from SOW/modeling, using chunked ingestion from object storage.\
\ls17\ilvl0
\f0\b \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	5	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Xact Components chunking and allocation
\f1\b0 : Implement chunked ingestion for Xact Components and a final allocation step, reusing parallelization infrastructure.\
\ls17\ilvl0
\f0\b \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	6	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Hardening and observability
\f1\b0 : Add richer 
\f2\fs26 metaJson
\f1\fs24 , per-phase metrics, and improved UI feedback. Tune chunk sizes and worker concurrency based on production load and performance tests.\
\pard\pardeftab720\sa240\partightenfactor0
\cf0 This staged approach delivers immediate robustness and scalability for Golden imports while building reusable infrastructure to support high-volume Xact CSV imports with true intra-file parallelism and durable web uploads.\
}